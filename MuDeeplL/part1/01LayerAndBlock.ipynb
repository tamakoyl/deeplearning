{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 层和块\n",
    "首先，我们回顾一下多层感知机"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2007,  0.1182, -0.0244, -0.2057,  0.0648,  0.2763, -0.0443, -0.0444,\n",
       "         -0.0720, -0.0116],\n",
       "        [-0.3654,  0.1575, -0.0320, -0.2817,  0.1067,  0.2986, -0.1514,  0.0854,\n",
       "         -0.0809,  0.0304]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "#nn.Sequential定义了一种特殊的Module\n",
    "net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "\n",
    "X = torch.rand(2, 20)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自定义块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)\n",
    "        self.out = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.out(F.relu(self.hidden(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实例化多层感知机的层，然后在每次调用正向传播函数时调用这些层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0284,  0.1635,  0.0455,  0.2417, -0.2252,  0.0130,  0.2973, -0.1710,\n",
       "          0.0131,  0.2003],\n",
       "        [ 0.1655,  0.1086,  0.0373,  0.2900, -0.1829, -0.0269,  0.2272, -0.0335,\n",
       "          0.0227,  0.1310]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "顺序块,nn.Sequential的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0533,  0.3883, -0.2857, -0.7450, -0.0244,  0.1967,  0.4143,  0.2408,\n",
       "         -0.2800,  0.3255, -0.3602,  0.3352,  0.7782, -0.2774,  0.0609,  0.3292,\n",
       "         -0.0427,  0.0765, -0.1772,  0.0892,  0.4317, -0.3205, -0.1823,  0.2161,\n",
       "         -0.3431, -0.3362, -0.1489, -0.2007,  0.3287,  0.4230,  0.4280, -0.9256,\n",
       "         -0.8958,  0.1628,  0.1933,  0.3426,  0.3092, -0.1803,  0.0559,  0.0452,\n",
       "         -0.5586,  0.3145,  0.1873,  0.6645,  0.2844, -0.6967,  0.1615, -0.6253,\n",
       "          0.1016,  0.1273,  1.1094, -0.0088,  0.2430,  0.5048,  0.6715, -0.3076,\n",
       "         -0.5530, -0.7185,  0.2848, -0.3977,  0.3643,  0.0758,  0.1437, -0.1576,\n",
       "         -0.3006,  0.8629, -0.0731, -0.2644,  0.0671,  0.3337,  0.2578,  0.0794,\n",
       "         -0.0065,  0.2746,  0.7139, -0.2100, -0.5257, -0.1971,  0.5081, -0.0463,\n",
       "          0.2966, -0.0130, -0.0269,  0.1048, -0.1602,  0.3705, -0.5501, -0.0079,\n",
       "         -0.2936, -0.1411, -0.1257, -0.1382, -0.0861,  0.4562,  0.3839, -0.1922,\n",
       "         -0.1138, -0.1762,  0.2197, -0.2615, -0.1773,  0.3876, -0.1035, -0.1960,\n",
       "         -0.0489,  0.0061,  0.3295, -0.4137,  0.3526,  0.0773, -0.0681,  0.4717,\n",
       "         -0.1289, -0.3654, -0.1312, -0.2211,  0.2785,  0.1949,  0.0657, -0.6169,\n",
       "         -0.2559, -0.4870,  0.0983, -0.2664,  0.1756, -0.1336,  0.5115,  0.4321,\n",
       "          0.2190, -0.0571,  0.3426,  0.3947, -1.1464, -0.1449, -0.0887, -0.4070,\n",
       "          0.4059,  0.1174,  0.0841, -0.7213,  0.1166,  0.3120, -0.0893, -0.2242,\n",
       "          0.6404, -0.2475,  0.0518,  0.9177, -0.2380, -0.6733, -0.2936,  0.3928,\n",
       "         -0.4016,  0.3039, -0.3886, -0.2237, -0.3417,  0.1212,  0.2263, -0.0682,\n",
       "          0.2679, -0.1985, -0.1522,  0.1522,  0.1961, -0.0042,  0.2970,  0.6827,\n",
       "          0.0966,  0.4612,  0.2539, -0.0852, -0.5193,  0.1568,  0.1090, -0.4072,\n",
       "         -1.1249,  0.4024,  0.8298,  0.3678, -0.0928, -0.2044,  0.1319, -0.2429,\n",
       "         -0.1425,  0.2063,  0.6115, -0.8278,  0.1594, -0.0323, -0.2171, -0.0731,\n",
       "          0.3873,  0.0536, -0.9153, -0.0960,  0.0084,  0.1230, -0.7440,  0.5484,\n",
       "          0.6520,  0.1335,  0.4779,  0.0763,  0.1261,  0.3237,  0.2897, -0.1396,\n",
       "         -0.4956, -0.1164, -0.5660,  0.2097,  0.5377,  0.1851,  0.0866,  0.3648,\n",
       "         -0.3268, -0.6151,  0.0621,  0.1618, -0.0751,  0.0106,  0.1649,  0.7859,\n",
       "          0.3199,  0.1346,  0.0391, -0.0861, -0.5326,  0.0570,  0.3442,  0.2158,\n",
       "          0.1174,  0.5103, -0.2975,  0.2121, -0.0628,  0.6936, -0.3273, -0.3419,\n",
       "         -0.5862, -0.1274,  0.5860,  0.4179,  0.2845,  0.0028, -0.0809, -0.0488,\n",
       "          0.1675, -0.0675,  0.0363, -0.0416,  0.2601, -0.2274,  0.5184,  0.0657],\n",
       "        [ 0.0686,  0.2698, -0.2255, -0.2952,  0.1029,  0.4181,  0.2215,  0.5416,\n",
       "         -0.1246,  0.3620, -0.1655,  0.5085,  0.6979,  0.0062,  0.0812,  0.0672,\n",
       "         -0.3627, -0.2520, -0.1228,  0.3087,  0.4487, -0.4379, -0.1292, -0.0848,\n",
       "         -0.5261, -0.3730,  0.0360,  0.0650, -0.2516,  0.2035,  0.0443, -0.6719,\n",
       "         -0.9314,  0.4605, -0.1290,  0.3615,  0.4166, -0.1556,  0.0511,  0.2411,\n",
       "         -0.6383,  0.4779,  0.3287,  0.4337,  0.1740, -0.3857,  0.6348, -0.4843,\n",
       "          0.0190,  0.0494,  1.1101,  0.2394,  0.2309,  0.1224,  0.5588, -0.4092,\n",
       "         -0.3632, -0.8365,  0.1098, -0.0040,  0.3799, -0.3324,  0.1239, -0.3610,\n",
       "         -0.4688,  0.8295, -0.1017, -0.2527,  0.3319,  0.3421,  0.1844, -0.0981,\n",
       "          0.0015,  0.3234,  0.5744, -0.1058, -0.2845,  0.1521,  0.6427, -0.1028,\n",
       "          0.2683, -0.1359,  0.2837,  0.1673, -0.1359,  0.2639, -0.7143,  0.1537,\n",
       "         -0.2619,  0.0412,  0.0532, -0.0881, -0.0847,  0.4240,  0.2021,  0.0037,\n",
       "         -0.0924,  0.0033,  0.4415,  0.0052, -0.0286,  0.3083, -0.2610,  0.3129,\n",
       "         -0.0052,  0.1449,  0.3233, -0.1872,  0.5264,  0.0019, -0.5032,  0.1378,\n",
       "          0.0169, -0.4904,  0.3355, -0.0482,  0.2465,  0.3128,  0.3734, -0.7106,\n",
       "         -0.2761, -0.1829,  0.1383, -0.2549,  0.0157, -0.3396,  0.3336, -0.0713,\n",
       "         -0.3487,  0.0826,  0.0343,  0.2758, -1.0599, -0.1976, -0.3486, -0.3317,\n",
       "          0.1501, -0.0717,  0.2485, -0.6255,  0.1303,  0.6151, -0.3569, -0.1099,\n",
       "          0.6186, -0.1665, -0.2204,  0.8376, -0.4023, -0.5549, -0.5332,  0.3166,\n",
       "         -0.3158,  0.3661, -0.2900, -0.1321,  0.0173,  0.1438,  0.2792,  0.1171,\n",
       "          0.2427, -0.3316, -0.2133,  0.0055,  0.3840, -0.1645,  0.2992,  0.6607,\n",
       "          0.0267,  0.4515,  0.1376, -0.2946, -0.2709,  0.0875,  0.2917, -0.1592,\n",
       "         -1.1261,  0.2798,  0.9330,  0.3209, -0.3125,  0.0063,  0.2396, -0.2637,\n",
       "          0.2013,  0.4421,  0.5285, -0.5507,  0.0171, -0.2419, -0.1710, -0.2762,\n",
       "          0.2969, -0.0790, -0.8729, -0.0995, -0.3935,  0.5348, -0.7850,  0.5045,\n",
       "          0.5572,  0.3757,  0.4708, -0.1154,  0.3292,  0.1756,  0.4981, -0.1118,\n",
       "         -0.5860, -0.3178, -0.3387,  0.1986,  0.1590, -0.0743,  0.0139,  0.3907,\n",
       "          0.0253, -0.1494,  0.1499,  0.2686, -0.0412, -0.1066,  0.3531,  0.5440,\n",
       "          0.4911, -0.1985,  0.3011, -0.1701, -0.1280,  0.3240,  0.4838,  0.3684,\n",
       "          0.3537,  0.4261, -0.4882,  0.5247,  0.1335,  0.9156, -0.5631, -0.4463,\n",
       "         -0.4632, -0.6353,  0.7599,  0.4757,  0.2973,  0.0761,  0.0653, -0.1577,\n",
       "          0.0820, -0.2804, -0.2855,  0.1854, -0.0638, -0.1595,  0.3051, -0.3438]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MySequential(nn.Module):\n",
    "    #传入的是一个list\n",
    "    def __init__(self,*args):\n",
    "        super().__init__()\n",
    "        for block in args:\n",
    "            #_modules是一个特殊的成员变量，是一个ordered dict\n",
    "            self._modules[block] = block\n",
    "    \n",
    "    def forward(self,X):\n",
    "        for block in self._modules.values():\n",
    "            X = block(X)\n",
    "            return X\n",
    "\n",
    "net = MySequential(nn.Linear(20,256),nn.ReLU(),nn.Linear(256,10))\n",
    "net(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在正向传播函数中执行代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0925, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 随机的weight 不需要计算梯度\n",
    "        self.rand_weight = torch.rand((20, 20), requires_grad=False)\n",
    "        self.linear = nn.Linear(20, 20)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        X = F.relu(torch.mm(X, self.rand_weight) + 1)\n",
    "        X = self.linear(X)\n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()\n",
    "\n",
    "net = FixedHiddenMLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "混合搭配各种组合块的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2044, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),\n",
    "                                 nn.Linear(64, 32), nn.ReLU())\n",
    "        self.linear = nn.Linear(32, 16)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())\n",
    "chimera(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2e6dea04d1ee67d02776821fdeeb43d084fbfe2bd3d12fe23449057b6c79404a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
